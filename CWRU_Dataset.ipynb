{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "\n",
    "\n",
    "from one_cycle import OneCycle, update_lr, update_mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = Path()\n",
    "normal_path = working_dir / 'Data' / 'Normal'\n",
    "DE_path = working_dir / 'Data' / '12k_DE'\n",
    "FE_path = working_dir / 'Data' / '12k_FE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matfile_to_dic(folder_path):\n",
    "    output_dic = {}\n",
    "    for i, filepath in enumerate(folder_path.glob('*.mat')):\n",
    "        key_name = str(filepath).split('\\\\')[-1]    #strip the folder path and get the filename only.\n",
    "        output_dic[key_name] = scipy.io.loadmat(filepath)\n",
    "    return output_dic\n",
    "\n",
    "def remove_dic_items(dic):\n",
    "    for key, values in dic.items():\n",
    "        del values['__header__']\n",
    "        del values['__version__']    \n",
    "        del values['__globals__']\n",
    "        \n",
    "def rename_keys(dic):\n",
    "    for k1,v1 in dic.items():\n",
    "        for k2,v2 in list(v1.items()):\n",
    "            if 'DE_time' in k2:\n",
    "                v1['DE_time'] = v1.pop(k2)\n",
    "            elif 'BA_time' in k2:\n",
    "                v1['BA_time'] = v1.pop(k2)\n",
    "            elif 'FE_time' in k2:\n",
    "                v1['FE_time'] = v1.pop(k2)\n",
    "            elif 'RPM' in k2:\n",
    "                v1['RPM'] = v1.pop(k2)\n",
    "                \n",
    "def label(x):\n",
    "    if 'B' in x:\n",
    "        return 'B'\n",
    "    elif 'IR' in x:\n",
    "        return 'IR'\n",
    "    elif 'OR' in x:\n",
    "        return 'OR'\n",
    "    elif 'Normal' in x:\n",
    "        return 'N'\n",
    "    \n",
    "def matfile_to_df(folder_path):\n",
    "    dic = matfile_to_dic(folder_path)\n",
    "    remove_dic_items(dic)\n",
    "    rename_keys(dic)\n",
    "    df = pd.DataFrame.from_dict(dic).T\n",
    "    df = df.reset_index().rename(mapper={'index':'filename'},axis=1)\n",
    "    df['label'] = df['filename'].apply(label)\n",
    "    return df.drop(['BA_time','FE_time', 'RPM', 'ans'], axis=1, errors='ignore')\n",
    "\n",
    "def divide_signal(df, segment_length):\n",
    "    dic = {}\n",
    "    idx = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        n_sample_points = len(df.iloc[i,1])\n",
    "        n_segments = n_sample_points // segment_length\n",
    "        for segment in range(n_segments):\n",
    "            dic[idx] = {\n",
    "                'signal': df.iloc[i,1][segment_length * segment:segment_length * (segment+1)], \n",
    "                'label': df.iloc[i,2],\n",
    "                'filename' : df.iloc[i,0]\n",
    "            }\n",
    "            idx += 1\n",
    "    df_tmp = pd.DataFrame.from_dict(dic,orient='index')\n",
    "    \n",
    "    return pd.concat([df_tmp[['label', 'filename']], pd.DataFrame(np.hstack(df_tmp[\"signal\"].values).T)], axis=1 )\n",
    "\n",
    "def get_df_all(normal_path, DE_path, segment_length=512):\n",
    "    df_Normal = matfile_to_df(normal_path)\n",
    "    df_DE = matfile_to_df(DE_path)\n",
    "    df_normal_processed = divide_signal(df_Normal, segment_length)\n",
    "    df_faulty_processed = divide_signal(df_DE, segment_length)\n",
    "    df_all = pd.concat([df_normal_processed, df_faulty_processed], axis=0, ignore_index=True)\n",
    "    map_label = {'N':0, 'B':1, 'IR':2, 'OR':3}\n",
    "    df_all['label'] = df_all['label'].map(map_label)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Normal = matfile_to_df(normal_path)\n",
    "df_DE = matfile_to_df(DE_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_plots = 10\n",
    "# df_plot = df_DE.sample(n_plots)\n",
    "# plt.figure(figsize=(15, 3*n_plots))\n",
    "# for i in range(n_plots):\n",
    "#     plt.subplot(n_plots, 1, i+1)\n",
    "#     plt.plot(df_plot.iloc[i,1])\n",
    "#     plt.ylim(-12, 12)\n",
    "#     plt.title(df_plot.iloc[i,0])\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = get_df_all(normal_path, DE_path, segment_length=5096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5096\n",
      "(1718, 5098)\n"
     ]
    }
   ],
   "source": [
    "features = df_all.columns[2:]\n",
    "target = 'label'\n",
    "print(len(features))\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(f'EPOCH {epoch}: \\t', val_loss)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit2(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    print('EPOCH', '\\t', 'Val Loss', '\\t', 'Accuracy')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = [loss_func(model(xb), yb) for xb, yb in valid_dl]\n",
    "            loss = torch.stack(loss, dim=0).numpy()\n",
    "            predictions = [torch.argmax(model(xb), dim=1) for xb, yb in valid_dl]\n",
    "#             set_trace()\n",
    "            predictions = torch.cat(predictions, dim=0).numpy()\n",
    "#         set_trace()\n",
    "        val_loss = np.mean(loss)\n",
    "        accuracy = np.mean((predictions == valid_dl.dataset.tensors[1].numpy()))\n",
    "\n",
    "        print(f'{epoch}: \\t', f'{val_loss:.05f}', '\\t', f'{accuracy:.05f}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit_one_cycle(epochs, model, loss_func, opt, train_dl, valid_dl, one_cycle_scheduler):\n",
    "    print('EPOCH', '\\t', 'Val Loss', '\\t', 'Accuracy')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "            lr, mom = onecycle.calc()\n",
    "            update_lr(opt, lr)\n",
    "            update_mom(opt, mom)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = [loss_func(model(xb), yb) for xb, yb in valid_dl]\n",
    "            loss = torch.stack(loss, dim=0).numpy()\n",
    "            predictions = [torch.argmax(model(xb), dim=1) for xb, yb in valid_dl]\n",
    "#             set_trace()\n",
    "            predictions = torch.cat(predictions, dim=0).numpy()\n",
    "#         set_trace()\n",
    "        val_loss = np.mean(loss)\n",
    "        accuracy = np.mean((predictions == valid_dl.dataset.tensors[1].numpy()))\n",
    "\n",
    "        print(f'{epoch}: \\t', f'{val_loss:.05f}', '\\t', f'{accuracy:.05f}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_length 1000, lowest loss 0.011\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.conv_in = nn.Conv1d(1, 32, (9,), stride=1, padding=4)\n",
    "        self.conv_1 = nn.Conv1d(32, 64, (5,), stride=1, padding=2)\n",
    "        self.conv_2 = nn.Conv1d(64, 128, (5,), stride=1, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(2,stride=2)\n",
    "        self.averagepool = nn.AvgPool1d(2,stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear1 = nn.Linear(self.n_in*64 //4, 50)\n",
    "        self.linear2 = nn.Linear(50, 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = F.relu(self.conv_in(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.averagepool(x)\n",
    "        x = x.view(-1, self.n_in*64//4)\n",
    "        x = self.linear1(x)\n",
    "        return self.linear2(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_length 1000, lowest loss 0.014\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.conv_in = nn.Conv1d(1, 32, (9,), stride=1, padding=4)\n",
    "        self.conv_1 = nn.Conv1d(32, 64, (5,), stride=1, padding=2)\n",
    "        self.conv_2 = nn.Conv1d(64, 128, (5,), stride=1, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(2,stride=2)\n",
    "        self.averagepool = nn.AvgPool1d(2,stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear1 = nn.Linear(self.n_in*64 //4, 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = F.relu(self.conv_in(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.averagepool(x)\n",
    "        x = x.view(-1, self.n_in*64//4)\n",
    "        return self.linear1(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_length 1000, lowest loss \n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.conv_in = nn.Conv1d(1, 32, (9,), stride=1, padding=4)\n",
    "        self.conv_1 = nn.Conv1d(32, 64, (5,), stride=1, padding=2)\n",
    "        self.conv_2 = nn.Conv1d(64, 128, (5,), stride=1, padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(2,stride=2)\n",
    "        self.averagepool = nn.AvgPool1d(2,stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.linear1 = nn.Linear(self.n_in*64 //4, 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = self.dropout(F.relu(self.conv_in(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(F.relu(self.conv_1(x)))\n",
    "        x = self.averagepool(x)\n",
    "        x = x.view(-1, self.n_in*64//4)\n",
    "        return self.linear1(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, (9,), stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, (5,), stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, (5,), stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.n_in*128 //4, 50)\n",
    "        self.linear2 = nn.Linear(50, 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(-1, self.n_in*128//4)\n",
    "        x = self.linear1(x)\n",
    "        return self.linear2(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, (9,), stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, (5,), stride=1, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, (5,), stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.n_in*128 //4, 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(-1, self.n_in*128//4)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, (9,), stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, (5,), stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.AvgPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.n_in*128 //4, 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(-1, self.n_in*128//4)\n",
    "        return self.linear1(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, (9,), stride=1, padding=4),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, (5,), stride=1, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, (5,), stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool1d(2,stride=2)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.n_in*128 //8, 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(-1, self.n_in*128//8)\n",
    "        return self.linear1(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, n_in, layers = [1, 64, 64, 128]):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.layers = layers\n",
    "        for layer_num, layer_size in layers[:-1]:\n",
    "            setattr(f'layer_{layer_num + 1}') = nn.Sequential(\n",
    "                nn.Conv1d(layers[layer_num], layers[layer_num + 1], (5,), stride=1, padding=2),\n",
    "                nn.BatchNorm1d(layers[layer_num + 1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.MaxPool1d(2,stride=2)\n",
    "            )\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.n_in*layers[-1] // 2**(len(layers)-1), 4)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.n_in)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(-1, self.n_in*128//8)\n",
    "        return self.linear1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df_all[features], \n",
    "                                                      df_all[target], \n",
    "                                                      test_size=0.20, random_state=42, shuffle=True\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "bs = 64\n",
    "wd = 1e-3\n",
    "epochs = 20\n",
    "loss_func = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "valid_ds = TensorDataset(X_valid, y_valid)\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = CNN_1D(len(features))\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "# opt = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
    "model = fit2(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = CNN_1D(1000)\n",
    "# opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "opt = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
    "model = fit2(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "bs = 64\n",
    "wd = 1e-3\n",
    "epochs = 10\n",
    "model = fit2(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for xb, _ in valid_dl:\n",
    "        pred = F.softmax(model(xb),dim=1)\n",
    "        prob, pred = torch.max(pred,1)\n",
    "#         print(pred.shape)\n",
    "        predictions.append(pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_valid.numpy() == torch.cat(predictions, dim=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y_valid.numpy() == torch.cat(predictions, dim=0).numpy()))\n",
    "print(len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit One Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "bs = 64\n",
    "wd = 1e-3\n",
    "epochs = 10\n",
    "loss_func = CrossEntropyLoss()\n",
    "onecycle = OneCycle(int(len(X_train) * epochs / bs), lr, prcnt=(epochs - 82) * 100/epochs, momentum_vals=(0.95, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH \t Val Loss \t Accuracy\n",
      "0: \t 2.79584 \t 0.35240\n",
      "1: \t 0.70464 \t 0.77460\n",
      "2: \t 0.36010 \t 0.85240\n",
      "3: \t 0.26214 \t 0.88330\n",
      "4: \t 0.16237 \t 0.94050\n",
      "5: \t 0.48454 \t 0.82609\n",
      "6: \t 0.14245 \t 0.94279\n",
      "7: \t 0.33786 \t 0.85126\n",
      "8: \t 0.10058 \t 0.96453\n",
      "9: \t 0.08943 \t 0.96224\n",
      "Wall time: 10min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = CNN_1D(len(features))\n",
    "opt = optim.SGD(model.parameters(), lr=lr/10, momentum=0.95, weight_decay=wd)\n",
    "# opt = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=wd)\n",
    "model = fit_one_cycle(epochs, model, loss_func, opt, train_dl, valid_dl, onecycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
